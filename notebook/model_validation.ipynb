{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:50.657762Z",
     "start_time": "2024-05-21T07:52:48.309469Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sagemaker import evaluation\n",
    "from torch.utils import data\n",
    "\n",
    "from model.neu_mf import NeuMF\n",
    "from validation_dataset import ValidationDataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "5c6d0a303496245",
   "metadata": {},
   "source": [
    "#### Hyper-parameter와 그외 설정"
   ]
  },
  {
   "cell_type": "code",
   "id": "7fa1bf10d1f42fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:50.661579Z",
     "start_time": "2024-05-21T07:52:50.659047Z"
    }
   },
   "source": [
    "args = {\n",
    "    # dataset\n",
    "    \"negative_sample_ratio\": 4,\n",
    "    \n",
    "    # model\n",
    "    \"predictive_factor_num\": 8,\n",
    "    \"mlp_layer_num\": 3,\n",
    "    \n",
    "    # learning\n",
    "    \"epochs\": 60,\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 0.001,\n",
    "    \n",
    "    # evaluation\n",
    "    \"eval_k\": 10,\n",
    "}\n",
    "\n",
    "DEVICE = 'mps'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "3e51641038ee93ac",
   "metadata": {},
   "source": [
    "#### 사용자와 아이템 수 정의"
   ]
  },
  {
   "cell_type": "code",
   "id": "1582d09e6265ad64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:50.664291Z",
     "start_time": "2024-05-21T07:52:50.662465Z"
    }
   },
   "source": [
    "user_number = 943\n",
    "item_number = 1682"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ac551f24a7403fef",
   "metadata": {},
   "source": [
    "#### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "id": "ece56479dc5b73c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:50.671124Z",
     "start_time": "2024-05-21T07:52:50.665293Z"
    }
   },
   "source": [
    "class ImplicitSparseDataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def __create_negative_samples(M, N, sparse_Y_u, sparse_Y_i, sample_ratio):\n",
    "        sparse_Y = [(u.item(), i.item()) for (u, i) in zip(sparse_Y_u, sparse_Y_i)]\n",
    "\n",
    "        negative_samples = []\n",
    "        for u in range(M):\n",
    "            for _ in range(sample_ratio):\n",
    "                while True:\n",
    "                    i = np.random.randint(N)\n",
    "                    if (u, i) not in sparse_Y:\n",
    "                        break\n",
    "                negative_samples.append((u, i))\n",
    "\n",
    "        negative_samples = torch.tensor(negative_samples)\n",
    "        return negative_samples[:, 0], negative_samples[:, 1]\n",
    "\n",
    "    def __init__(self, M, N, sparse_Y_u, sparse_Y_i, include_negative_samples=False, negative_sample_ratio=1):\n",
    "        super(ImplicitSparseDataset, self).__init__()\n",
    "\n",
    "        assert len(sparse_Y_u) == len(sparse_Y_i), \"Length of `sparse_Y_u` and `sparse_Y_i` must be equal.\"\n",
    "\n",
    "        self.len = len(sparse_Y_u)\n",
    "        self.sparsity = 1 - (self.len / (M * N))\n",
    "\n",
    "        self.Y_u = sparse_Y_u\n",
    "        self.Y_i = sparse_Y_i\n",
    "        self.Y_value = torch.tensor([1.0] * self.len)\n",
    "\n",
    "        if include_negative_samples:\n",
    "            negative_sample_size = M * negative_sample_ratio\n",
    "            self.len = self.len + negative_sample_size\n",
    "\n",
    "            n_Y_u, n_Y_i = ImplicitSparseDataset.__create_negative_samples(M, N, self.Y_u, self.Y_i,\n",
    "                                                                           negative_sample_ratio)\n",
    "\n",
    "            self.Y_u = torch.cat((self.Y_u, n_Y_u), dim=0)\n",
    "            self.Y_i = torch.cat((self.Y_i, n_Y_i), dim=0)\n",
    "            self.Y_value = torch.cat((self.Y_value, torch.tensor([0.0] * negative_sample_size)), dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Y_u[idx], self.Y_i[idx], self.Y_value[idx]\n",
    "\n",
    "    def get_sparsity(self):\n",
    "        return self.sparsity"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "5d55588496a62429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:50.675629Z",
     "start_time": "2024-05-21T07:52:50.672841Z"
    }
   },
   "source": [
    "def create_dataset(train_csv, val_csv, _user_number, _item_number, negative_sample_ratio):\n",
    "    _train_dataset = pd.read_csv(train_csv, dtype={0: np.int32, 1: np.int32}, header=None)\n",
    "    _train_dataset = torch.from_numpy(_train_dataset.values)\n",
    "    _train_dataset = ImplicitSparseDataset(_user_number, _item_number, _train_dataset[:, 0], _train_dataset[:, 1],\n",
    "                                          include_negative_samples=True, negative_sample_ratio=negative_sample_ratio)\n",
    "\n",
    "    _val_dataset = ValidationDataset(val_csv)\n",
    "\n",
    "    return _train_dataset, _val_dataset"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "761c942447c860c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:56.873861Z",
     "start_time": "2024-05-21T07:52:50.789408Z"
    }
   },
   "source": [
    "train_dataset, val_dataset = create_dataset('../ml-100k.train.csv', '../ml-100k.val.csv',\n",
    "                                            user_number, item_number, args['negative_sample_ratio'])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=args[\"batch_size\"], shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=args[\"batch_size\"], shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "ca1efe41050d589c",
   "metadata": {},
   "source": [
    "#### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "id": "61d291becab35bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:56.906710Z",
     "start_time": "2024-05-21T07:52:56.875132Z"
    }
   },
   "source": [
    "model = NeuMF(\"neumf-model_validation\",\n",
    "              user_number, item_number,\n",
    "              predictive_factor_num=args[\"predictive_factor_num\"],\n",
    "              mlp_layer_num=args[\"mlp_layer_num\"], dropout_prob=0.3).to(DEVICE)\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuMF(\n",
       "  (mlp_P): Embedding(943, 16)\n",
       "  (mlp_Q): Embedding(1682, 16)\n",
       "  (mlp_layer_X): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Dropout(p=0.3, inplace=False)\n",
       "      (1): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Dropout(p=0.3, inplace=False)\n",
       "      (1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Dropout(p=0.3, inplace=False)\n",
       "      (1): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (gmf_P): Embedding(943, 4)\n",
       "  (gmf_Q): Embedding(1682, 4)\n",
       "  (neu_mf): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "38c1cfad90566980",
   "metadata": {},
   "source": [
    "\n",
    "#### 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f9ce11641bcbdf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:56.912568Z",
     "start_time": "2024-05-21T07:52:56.907638Z"
    }
   },
   "source": [
    "def train(run_name):\n",
    "    global train_loader\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=\"runs/\" + run_name)\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        start_time = time.time()\n",
    "    \n",
    "        batch = 0\n",
    "        loss_sum = 0\n",
    "        model.train() \n",
    "        for user, item, label in train_loader:\n",
    "            batch += 1\n",
    "    \n",
    "            user = user.to(DEVICE)\n",
    "            item = item.to(DEVICE)\n",
    "            label = label.reshape(-1, 1).to(DEVICE)\n",
    "    \n",
    "            model.zero_grad()\n",
    "            outputs = model(user, item)\n",
    "    \n",
    "            loss = criterion(outputs, label)\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        writer.add_scalar(\"train/loss\", loss_sum / batch, epoch + 1)\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            hr, mean_ap, norm_dcg = evaluation.evaluate(model, val_loader, args[\"eval_k\"], device=DEVICE)\n",
    "            writer.add_scalar(\"val/hr\", hr, epoch + 1)\n",
    "            writer.add_scalar(\"val/mAP\", mean_ap, epoch + 1)\n",
    "            writer.add_scalar(\"val/nDCG\", norm_dcg, epoch + 1)\n",
    "    \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"The time elapse of epoch {:03d} is: \".format(epoch + 1) + str(elapsed_time))"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "16692bf074022441",
   "metadata": {},
   "source": [
    "<b>Run 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7d6cefa54a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"model_validation-run1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f24b725b6bb95",
   "metadata": {},
   "source": [
    "![](img/model_validation-run1-train.png)<br/>\n",
    "![](img/model_validation-run1-val.png)<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b7acd7bfeee5a",
   "metadata": {},
   "source": [
    "기이하게도 학습을 거듭할수록 성능지표가 떨어지는 현상을 보였다. 비록 validation 성과가 반락하는 통상적인 overfitting의 모습은 아니였지만 train-loss가 수렴치를 갖지않고 0에 가까워 지도록 하향했다는 점에서 과적합을 의심해 보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c757b40c4a7688",
   "metadata": {},
   "source": [
    "우선 학습데이터셋의 negative sample이 positive instance에 비해 현저히 적었다는 점을 문제삼아 보았다. 논문에 의하면 positive instance 대 negative instance의 비율을 hyper-parameter로 정의하고 조절하였으나 Run1에서는 user 당 negative instance의 개수를 조절치로 두었다."
   ]
  },
  {
   "cell_type": "code",
   "id": "113bab558dab3e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:56.916748Z",
     "start_time": "2024-05-21T07:52:56.913998Z"
    }
   },
   "source": [
    "# Train Dataset Size\n",
    "len(train_loader.dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83772"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "9320b5d8159674ff",
   "metadata": {},
   "source": [
    "Update `ImplicitSparseDataset.__create_negative_samples`"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9cd1d374f30503f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:56.923703Z",
     "start_time": "2024-05-21T07:52:56.917768Z"
    }
   },
   "source": [
    "class ImplicitSparseDataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def __create_negative_samples(N, sparse_Y_u, sparse_Y_i, sample_ratio):\n",
    "        all_items = torch.arange(N, dtype=torch.int32)\n",
    "    \n",
    "        negative_samples = torch.tensor([], dtype=torch.int32)\n",
    "        \n",
    "        for user_id in sparse_Y_u.unique():\n",
    "            positive_items = sparse_Y_i[sparse_Y_u == user_id]\n",
    "            negative_sample_size = len(positive_items) * sample_ratio\n",
    "                \n",
    "            possible_negative_items = all_items[torch.isin(all_items, positive_items, invert=True)]\n",
    "            \n",
    "            random_indices = torch.randperm(len(possible_negative_items))[:negative_sample_size]\n",
    "            \n",
    "            negative_items = possible_negative_items[random_indices].unsqueeze(dim=1)\n",
    "            negative_items = torch.cat((torch.full((negative_items.size(0), 1), user_id), negative_items), dim=1)\n",
    "            \n",
    "            negative_samples = torch.cat((negative_samples, negative_items))\n",
    "        \n",
    "        return negative_samples[:, 0], negative_samples[:, 1]\n",
    "\n",
    "    def __init__(self, M, N, sparse_Y_u, sparse_Y_i, include_negative_samples=False, negative_sample_ratio=1):\n",
    "        super(ImplicitSparseDataset, self).__init__()\n",
    "\n",
    "        assert len(sparse_Y_u) == len(sparse_Y_i), \"Length of `sparse_Y_u` and `sparse_Y_i` must be equal.\"\n",
    "\n",
    "        self.len = len(sparse_Y_u)\n",
    "        self.sparsity = 1 - (self.len / (M * N))\n",
    "\n",
    "        self.Y_u = sparse_Y_u\n",
    "        self.Y_i = sparse_Y_i\n",
    "        self.Y_value = torch.tensor([1.0] * self.len)\n",
    "\n",
    "        if include_negative_samples:\n",
    "            n_Y_u, n_Y_i = ImplicitSparseDataset.__create_negative_samples(N, self.Y_u, self.Y_i, negative_sample_ratio)\n",
    "            negative_sample_size = n_Y_u.size(0)\n",
    "    \n",
    "            self.Y_u = torch.cat((self.Y_u, n_Y_u), dim=0)\n",
    "            self.Y_i = torch.cat((self.Y_i, n_Y_i), dim=0)\n",
    "            self.Y_value = torch.cat((self.Y_value, torch.tensor([0.0] * negative_sample_size)), dim=0)\n",
    "            self.len = self.len + negative_sample_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Y_u[idx], self.Y_i[idx], self.Y_value[idx]\n",
    "\n",
    "    def get_sparsity(self):\n",
    "        return self.sparsity"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "7e939df6b568380d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:58.143965Z",
     "start_time": "2024-05-21T07:52:56.924860Z"
    }
   },
   "source": [
    "train_dataset, val_dataset = create_dataset('../ml-100k.train.csv', '../ml-100k.val.csv',\n",
    "                                            user_number, item_number, args['negative_sample_ratio'])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=args[\"batch_size\"], shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=args[\"batch_size\"], shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "9ed49f9d9a4d51e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:58.147873Z",
     "start_time": "2024-05-21T07:52:58.144937Z"
    }
   },
   "source": [
    "# Train Dataset Size\n",
    "len(train_loader.dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394518"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "3e623c5be0427034",
   "metadata": {},
   "source": [
    "<b>Run 2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8a9eecaf9fbdaf4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T06:28:40.118391Z",
     "start_time": "2024-05-20T05:51:50.686636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapse of epoch 001 is: 43.537853956222534\n",
      "The time elapse of epoch 002 is: 36.81234622001648\n",
      "The time elapse of epoch 003 is: 38.75852084159851\n",
      "The time elapse of epoch 004 is: 36.98269605636597\n",
      "The time elapse of epoch 005 is: 37.29785394668579\n",
      "The time elapse of epoch 006 is: 36.2294819355011\n",
      "The time elapse of epoch 007 is: 37.140844106674194\n",
      "The time elapse of epoch 008 is: 36.70030212402344\n",
      "The time elapse of epoch 009 is: 36.600587129592896\n",
      "The time elapse of epoch 010 is: 35.73583912849426\n",
      "The time elapse of epoch 011 is: 36.8011257648468\n",
      "The time elapse of epoch 012 is: 36.28961396217346\n",
      "The time elapse of epoch 013 is: 36.05220985412598\n",
      "The time elapse of epoch 014 is: 37.043025970458984\n",
      "The time elapse of epoch 015 is: 37.14964032173157\n",
      "The time elapse of epoch 016 is: 36.62901711463928\n",
      "The time elapse of epoch 017 is: 35.4528911113739\n",
      "The time elapse of epoch 018 is: 34.6025128364563\n",
      "The time elapse of epoch 019 is: 36.737536907196045\n",
      "The time elapse of epoch 020 is: 35.726938009262085\n",
      "The time elapse of epoch 021 is: 36.769089221954346\n",
      "The time elapse of epoch 022 is: 36.813286781311035\n",
      "The time elapse of epoch 023 is: 38.882246017456055\n",
      "The time elapse of epoch 024 is: 35.567039012908936\n",
      "The time elapse of epoch 025 is: 34.74007201194763\n",
      "The time elapse of epoch 026 is: 35.12674880027771\n",
      "The time elapse of epoch 027 is: 34.78721594810486\n",
      "The time elapse of epoch 028 is: 34.66920876502991\n",
      "The time elapse of epoch 029 is: 34.729491233825684\n",
      "The time elapse of epoch 030 is: 34.756345987319946\n",
      "The time elapse of epoch 031 is: 35.362059116363525\n",
      "The time elapse of epoch 032 is: 35.42706799507141\n",
      "The time elapse of epoch 033 is: 35.60198497772217\n",
      "The time elapse of epoch 034 is: 35.51917004585266\n",
      "The time elapse of epoch 035 is: 36.208032846450806\n",
      "The time elapse of epoch 036 is: 36.786566972732544\n",
      "The time elapse of epoch 037 is: 37.1254620552063\n",
      "The time elapse of epoch 038 is: 36.41104793548584\n",
      "The time elapse of epoch 039 is: 37.54392600059509\n",
      "The time elapse of epoch 040 is: 39.26838564872742\n",
      "The time elapse of epoch 041 is: 37.99716901779175\n",
      "The time elapse of epoch 042 is: 37.142560958862305\n",
      "The time elapse of epoch 043 is: 35.54174995422363\n",
      "The time elapse of epoch 044 is: 35.38966703414917\n",
      "The time elapse of epoch 045 is: 36.47632718086243\n",
      "The time elapse of epoch 046 is: 36.52987790107727\n",
      "The time elapse of epoch 047 is: 38.25563383102417\n",
      "The time elapse of epoch 048 is: 40.13331890106201\n",
      "The time elapse of epoch 049 is: 36.716548919677734\n",
      "The time elapse of epoch 050 is: 36.94468879699707\n",
      "The time elapse of epoch 051 is: 37.84992170333862\n",
      "The time elapse of epoch 052 is: 36.59575414657593\n",
      "The time elapse of epoch 053 is: 37.729037046432495\n",
      "The time elapse of epoch 054 is: 37.48422718048096\n",
      "The time elapse of epoch 055 is: 37.81389904022217\n",
      "The time elapse of epoch 056 is: 37.83207607269287\n",
      "The time elapse of epoch 057 is: 37.359018087387085\n",
      "The time elapse of epoch 058 is: 37.81770396232605\n",
      "The time elapse of epoch 059 is: 38.82307004928589\n",
      "The time elapse of epoch 060 is: 38.58797311782837\n"
     ]
    }
   ],
   "source": [
    "train(\"model_validation-run2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7647fdd265e6fa",
   "metadata": {},
   "source": [
    "![](img/model_validation-run2-train.png)<br/>\n",
    "![](img/model_validation-run2-val.png)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9f0b1dcd45a8",
   "metadata": {},
   "source": [
    "이번에는 정상적으로 학습이 되는 것을 확인하였지만 HR@10, mAP@10, nDCG@10의 최고점이 각각 `0.6182`, `0.3011`, `0.3758`로 논문의 결과보다 떨어진다.<br />\n",
    "따라서 이번에는 매 epoch마다 negative sampling을 새로 생성하도록 변화를 주어 학습을 시도해본다.<br />"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb7acf45c4461756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:58.859412Z",
     "start_time": "2024-05-21T07:52:58.852655Z"
    }
   },
   "source": [
    "class ImplicitSparseDataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def __create_negative_samples(N, sparse_Y_u, sparse_Y_i, sample_ratio):\n",
    "        all_items = torch.arange(N, dtype=torch.int32)\n",
    "    \n",
    "        negative_samples = torch.tensor([], dtype=torch.int32)\n",
    "        \n",
    "        for user_id in sparse_Y_u.unique():\n",
    "            positive_items = sparse_Y_i[sparse_Y_u == user_id]\n",
    "            negative_sample_size = len(positive_items) * sample_ratio\n",
    "                \n",
    "            possible_negative_items = all_items[torch.isin(all_items, positive_items, invert=True)]\n",
    "            \n",
    "            random_indices = torch.randperm(len(possible_negative_items))[:negative_sample_size]\n",
    "            \n",
    "            negative_items = possible_negative_items[random_indices].unsqueeze(dim=1)\n",
    "            negative_items = torch.cat((torch.full((negative_items.size(0), 1), user_id), negative_items), dim=1)\n",
    "            \n",
    "            negative_samples = torch.cat((negative_samples, negative_items))\n",
    "        \n",
    "        return negative_samples[:, 0], negative_samples[:, 1]\n",
    "\n",
    "    def __init__(self, M, N, sparse_Y_u, sparse_Y_i, negative_sample_ratio=1):\n",
    "        super(ImplicitSparseDataset, self).__init__()\n",
    "\n",
    "        assert len(sparse_Y_u) == len(sparse_Y_i), \"Length of `sparse_Y_u` and `sparse_Y_i` must be equal.\"\n",
    "        \n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.negative_sample_ratio = negative_sample_ratio\n",
    "\n",
    "        self.p_Y_u = sparse_Y_u\n",
    "        self.p_Y_i = sparse_Y_i\n",
    "        self.p_len = len(self.p_Y_u)\n",
    "        self.p_Y_value = torch.tensor([1.0] * self.p_len)\n",
    "        \n",
    "        self.Y_u = self.p_Y_u\n",
    "        self.Y_i = self.p_Y_i\n",
    "        self.len = self.p_len\n",
    "        self.Y_value = self.p_Y_value\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Y_u[idx], self.Y_i[idx], self.Y_value[idx]\n",
    "    \n",
    "    def regenerate_negative_samples(self):\n",
    "        n_Y_u, n_Y_i = ImplicitSparseDataset.__create_negative_samples(self.N, self.p_Y_u, self.p_Y_i, self.negative_sample_ratio)\n",
    "        n_len = n_Y_u.size(0)\n",
    "        n_Y_value = torch.tensor([0.0] * n_len)\n",
    "        \n",
    "        self.Y_u = torch.cat((self.p_Y_u, n_Y_u), dim=0)\n",
    "        self.Y_i = torch.cat((self.p_Y_i, n_Y_i), dim=0)\n",
    "        self.len = self.p_len + n_len\n",
    "        self.Y_value = torch.cat((self.p_Y_value, n_Y_value), dim=0)\n",
    "\n",
    "    def get_sparsity(self):\n",
    "        return 1 - (self.len / (self.M * self.N))"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "9366ed83443505f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:52:59.255377Z",
     "start_time": "2024-05-21T07:52:59.252106Z"
    }
   },
   "source": [
    "def create_dataset(train_csv, val_csv, _user_number, _item_number, negative_sample_ratio):\n",
    "    _train_dataset = pd.read_csv(train_csv, dtype={0: np.int32, 1: np.int32}, header=None)\n",
    "    _train_dataset = torch.from_numpy(_train_dataset.values)\n",
    "    _train_dataset = ImplicitSparseDataset(_user_number, _item_number, _train_dataset[:, 0], _train_dataset[:, 1], negative_sample_ratio=negative_sample_ratio)\n",
    "\n",
    "    _val_dataset = ValidationDataset(val_csv)\n",
    "\n",
    "    return _train_dataset, _val_dataset"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "f4e80d89847ba403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:53:00.226831Z",
     "start_time": "2024-05-21T07:52:59.539272Z"
    }
   },
   "source": [
    "train_dataset, val_dataset = create_dataset('../ml-100k.train.csv', '../ml-100k.val.csv',\n",
    "                                            user_number, item_number, args['negative_sample_ratio'])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=args[\"batch_size\"], shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=args[\"batch_size\"], shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "536778e6a704cb95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:53:00.258467Z",
     "start_time": "2024-05-21T07:53:00.255689Z"
    }
   },
   "source": [
    "# Train Dataset Size (before generating negative samples)\n",
    "len(train_loader.dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "29c6651e4ca8b0fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:53:01.058038Z",
     "start_time": "2024-05-21T07:53:00.500891Z"
    }
   },
   "source": [
    "# Train Dataset Size (after generating negative samples)\n",
    "train_loader.dataset.regenerate_negative_samples()\n",
    "len(train_loader.dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394518"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "c35c2dea61c47443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:53:01.211846Z",
     "start_time": "2024-05-21T07:53:01.203411Z"
    }
   },
   "source": [
    "def train(run_name):\n",
    "    global train_loader\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=\"runs/\" + run_name)\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        start_time = time.time()\n",
    "    \n",
    "        batch = 0\n",
    "        loss_sum = 0\n",
    "        model.train() \n",
    "        train_loader.dataset.regenerate_negative_samples()\n",
    "        for user, item, label in train_loader:\n",
    "            batch += 1\n",
    "    \n",
    "            user = user.to(DEVICE)\n",
    "            item = item.to(DEVICE)\n",
    "            label = label.reshape(-1, 1).to(DEVICE)\n",
    "    \n",
    "            model.zero_grad()\n",
    "            outputs = model(user, item)\n",
    "    \n",
    "            loss = criterion(outputs, label)\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        writer.add_scalar(\"train/loss\", loss_sum / batch, epoch + 1)\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            hr, mean_ap, norm_dcg = evaluation.evaluate(model, val_loader, args[\"eval_k\"], device=DEVICE)\n",
    "            writer.add_scalar(\"val/hr\", hr, epoch + 1)\n",
    "            writer.add_scalar(\"val/mAP\", mean_ap, epoch + 1)\n",
    "            writer.add_scalar(\"val/nDCG\", norm_dcg, epoch + 1)\n",
    "    \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"The time elapse of epoch {:03d} is: \".format(epoch + 1) + str(elapsed_time))"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "dc277bfe6bdf961",
   "metadata": {},
   "source": [
    "<b>Run 3</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "322944011edbf0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T07:45:56.552276Z",
     "start_time": "2024-05-20T07:08:09.724455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapse of epoch 001 is: 37.58870720863342\n",
      "The time elapse of epoch 002 is: 37.18972325325012\n",
      "The time elapse of epoch 003 is: 37.77529287338257\n",
      "The time elapse of epoch 004 is: 38.025829792022705\n",
      "The time elapse of epoch 005 is: 39.96527576446533\n",
      "The time elapse of epoch 006 is: 40.833390951156616\n",
      "The time elapse of epoch 007 is: 39.278671979904175\n",
      "The time elapse of epoch 008 is: 37.38867998123169\n",
      "The time elapse of epoch 009 is: 37.829949378967285\n",
      "The time elapse of epoch 010 is: 37.66372084617615\n",
      "The time elapse of epoch 011 is: 36.48789596557617\n",
      "The time elapse of epoch 012 is: 36.897764921188354\n",
      "The time elapse of epoch 013 is: 38.004019021987915\n",
      "The time elapse of epoch 014 is: 38.76742506027222\n",
      "The time elapse of epoch 015 is: 38.22804021835327\n",
      "The time elapse of epoch 016 is: 37.607645988464355\n",
      "The time elapse of epoch 017 is: 38.61990189552307\n",
      "The time elapse of epoch 018 is: 37.97487688064575\n",
      "The time elapse of epoch 019 is: 37.43840193748474\n",
      "The time elapse of epoch 020 is: 38.78956890106201\n",
      "The time elapse of epoch 021 is: 37.55285286903381\n",
      "The time elapse of epoch 022 is: 37.80568504333496\n",
      "The time elapse of epoch 023 is: 37.23430299758911\n",
      "The time elapse of epoch 024 is: 37.344895124435425\n",
      "The time elapse of epoch 025 is: 37.628005027770996\n",
      "The time elapse of epoch 026 is: 37.65137529373169\n",
      "The time elapse of epoch 027 is: 38.64141321182251\n",
      "The time elapse of epoch 028 is: 38.898529052734375\n",
      "The time elapse of epoch 029 is: 36.538450956344604\n",
      "The time elapse of epoch 030 is: 36.22548198699951\n",
      "The time elapse of epoch 031 is: 36.36905908584595\n",
      "The time elapse of epoch 032 is: 37.33505201339722\n",
      "The time elapse of epoch 033 is: 36.90834927558899\n",
      "The time elapse of epoch 034 is: 36.71434712409973\n",
      "The time elapse of epoch 035 is: 36.63077402114868\n",
      "The time elapse of epoch 036 is: 37.17751216888428\n",
      "The time elapse of epoch 037 is: 37.26928400993347\n",
      "The time elapse of epoch 038 is: 36.837599992752075\n",
      "The time elapse of epoch 039 is: 37.41917395591736\n",
      "The time elapse of epoch 040 is: 37.48539090156555\n",
      "The time elapse of epoch 041 is: 36.265575885772705\n",
      "The time elapse of epoch 042 is: 37.09672403335571\n",
      "The time elapse of epoch 043 is: 37.30573320388794\n",
      "The time elapse of epoch 044 is: 36.98737192153931\n",
      "The time elapse of epoch 045 is: 37.69538378715515\n",
      "The time elapse of epoch 046 is: 36.618175983428955\n",
      "The time elapse of epoch 047 is: 37.46533012390137\n",
      "The time elapse of epoch 048 is: 38.04175114631653\n",
      "The time elapse of epoch 049 is: 40.58921790122986\n",
      "The time elapse of epoch 050 is: 39.46850299835205\n",
      "The time elapse of epoch 051 is: 39.01119303703308\n",
      "The time elapse of epoch 052 is: 37.92954778671265\n",
      "The time elapse of epoch 053 is: 37.80117988586426\n",
      "The time elapse of epoch 054 is: 38.55397701263428\n",
      "The time elapse of epoch 055 is: 37.53559422492981\n",
      "The time elapse of epoch 056 is: 37.69243311882019\n",
      "The time elapse of epoch 057 is: 37.96037411689758\n",
      "The time elapse of epoch 058 is: 38.44654178619385\n",
      "The time elapse of epoch 059 is: 38.175081968307495\n",
      "The time elapse of epoch 060 is: 38.13336515426636\n"
     ]
    }
   ],
   "source": [
    "train(\"model_validation-run3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b196f2e723a7dd5",
   "metadata": {},
   "source": [
    "![](img/model_validation-run3-train.png)<br/>\n",
    "![](img/model_validation-run3-val.png)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a27215be17718",
   "metadata": {},
   "source": [
    "HR@10: `0.6336`<br/>\n",
    "mAP@10: `0.316`<br/>\n",
    "nDCG@10: `0.3905`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3447e62b5aa5362",
   "metadata": {},
   "source": "여전히 논문의 결과(HR@10=`0.671`, nDCG@10=`0.399`)에 살짝 못미치지만 데이터세트가 논문에서 사용한 1M이 아닌 100K 데이터셋이라는 점, hyper-parameter tuning이 되기전 최초 학습이라는 점, 그리고 마지막 layer의 factor를 최소로 잡고 학습을 시켰다는 점 등을 고려하면 개선 작업을 통하여 서비스에 활용해볼 가치는 충분하다고 생각된다."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4f4de6de7d8521c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
